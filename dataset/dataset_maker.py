import os
import json
import random

from openpyxl import load_workbook

#ORGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‚ç…§ã—ã€Train/validç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å†åˆ†é…
class DatasetMaker:
    def __init__(self, makedate, header_row=1, train_ratio=0.8, seed=42):
        self.header_row = header_row
        self.train_ratio = train_ratio
        self.seed = seed
        self.Reffolder_dir = os.path.join(os.path.dirname(__file__), "ORG")
        self.jsonl_dir = os.path.join(os.path.dirname(__file__), makedate)
        self.dataset_dir = os.path.join(self.jsonl_dir, f"seed{seed}_train{int(train_ratio*100)}_valid{int((1-train_ratio)*100)}")
        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚‹ã‹ç¢ºèª
        self.Dir_exists = False
        if os.path.exists(self.dataset_dir):
            self.Dir_exists = True
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ(ãªã‘ã‚Œã°)
        os.makedirs(self.jsonl_dir, exist_ok=True)
        os.makedirs(self.dataset_dir, exist_ok=True)

    def main(self):
        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚Œã°ä½¿ç”¨
        if self.Dir_exists:
            print(f"æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™: {self.dataset_dir}")
            return

        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç„¡ã‘ã‚Œã°æ–°è¦ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
        print(f"æ–°è¦ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™: {self.dataset_dir}")
        #QAå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
        QAjsonl_path = self.ORG_to_dataset(ReffileType="QA")
        #Textå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
        Textjsonl_path = self.ORG_to_dataset(ReffileType="Text")
        
        #ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§
        jsonl_paths = [QAjsonl_path, Textjsonl_path]

        #QAå½¢å¼ã¨Textå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆ(å‚è€ƒç”¨)
        self.merge_selected_jsonl_files(jsonl_paths)
        
        #QAå½¢å¼ã¨Textå½¢å¼ã‹ã‚‰ä»»æ„ã®å‰²åˆã§åˆ†é…ã—ãŸtrain/validç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
        self.split_jsonl_files(jsonl_paths)

    def ORG_to_dataset(self, ReffileType="QA"):
        #å‚ç…§å…ˆç¢ºèª
        Reffolder_dir = os.path.join(self.Reffolder_dir, ReffileType)
        #å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_path = os.path.join(self.jsonl_dir, f"{ReffileType}_dataset.jsonl")
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        with open(output_path, 'w', encoding='utf-8') as jsonl_file:
            # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã™ã¹ã¦ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            for filename in os.listdir(Reffolder_dir):
                if filename.endswith((".xlsx",".xls",".xlsm")):
                    file_path = os.path.join(Reffolder_dir, filename)
                    wb = load_workbook(file_path, data_only=True)  # ãƒã‚¯ãƒ­ã¯ç„¡è¦–ã€å€¤ã®ã¿å–å¾—
                    # ã™ã¹ã¦ã®ã‚·ãƒ¼ãƒˆã‚’å‡¦ç†
                    for sheet_name in wb.sheetnames:
                        ws = wb[sheet_name]

                        # ---- è¦‹å‡ºã—è¡Œã‹ã‚‰åˆ—ã‚’ç‰¹å®š ----
                        header_cells = ws[self.header_row]
                        if ReffileType == "QA":
                            input_col = None
                            output_col = None
                        elif ReffileType == "Text":
                            text_col = None
                        
                        for cell in header_cells:
                            if cell.value is None:
                                continue
                            header_text = str(cell.value).lower()

                            if ReffileType == "QA":
                                if any(keyword in header_text for keyword in ["è³ªå•", "question", "prompt", "input"]):
                                    input_col = cell.column_letter
                                elif any(keyword in header_text for keyword in ["å›ç­”", "answer", "response", "output"]):
                                    output_col = cell.column_letter
                            elif ReffileType == "Text":
                                if any(keyword in header_text for keyword in ["æœ¬æ–‡", "åŸæ–‡", "text"]):
                                    text_col = cell.column_letter
                        
                        #---- å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ— ----
                        if ReffileType == "QA":
                            if not input_col or not output_col:
                                print(f"[è­¦å‘Š] {filename} ã® {sheet_name} ã§é©åˆ‡ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                                continue
                        elif ReffileType == "Text":
                            if not text_col:
                                print(f"[è­¦å‘Š] {filename} ã® {sheet_name} ã§é©åˆ‡ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                                continue

                        # ---- ãƒ‡ãƒ¼ã‚¿è¡Œã‚’èª­ã¿è¾¼ã¿ ----
                        row = self.header_row + 1
                        while True:
                            if ReffileType == "QA":
                                cell_input = ws[f'{input_col}{row}'].value
                                cell_output = ws[f'{output_col}{row}'].value
                                # çµ‚äº†æ¡ä»¶ï¼ˆä¸¡æ–¹ç©ºãªã‚‰çµ‚äº†ï¼‰
                                if cell_input is None and cell_output is None:
                                    break
                                record = {
                                    "input": cell_input if cell_input is not None else "",
                                    "output": cell_output if cell_output is not None else "",
                                }
                            elif ReffileType == "Text":
                                cell_text = ws[f'{text_col}{row}'].value
                                # çµ‚äº†æ¡ä»¶ï¼ˆç©ºãªã‚‰çµ‚äº†ï¼‰
                                if cell_text is None:
                                    break
                                record = {
                                    "text": cell_text if cell_text is not None else "",
                                }

                            # JSONLã«æ›¸ãè¾¼ã¿
                            jsonl_file.write(json.dumps(record, ensure_ascii=False) + '\n')
                            row += 1

        print(f"å®Œäº†ï¼š{output_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        return output_path

    def merge_selected_jsonl_files(self, jsonl_paths):
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_path = os.path.join(self.jsonl_dir, "All_dataset.jsonl")
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ
        with open(output_path, "w", encoding="utf-8") as outfile:
            for file_path in jsonl_paths:
                if not os.path.exists(file_path):
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                    continue

                with open(file_path, "r", encoding="utf-8") as infile:
                    for line_num, line in enumerate(infile, start=1):
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            json.loads(line)  # JSONã¨ã—ã¦æœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
                            outfile.write(line + "\n")
                        except json.JSONDecodeError:
                            print(f"âš ï¸ ç„¡åŠ¹ãªJSONè¡Œï¼ˆ{file_path}:{line_num}ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                            continue

        print(f"âœ… {len(jsonl_paths)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¾ã—ãŸ â†’ {output_path}")

    def split_jsonl_files(self, jsonl_paths):
        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š
        random.seed(self.seed)
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        train_output_path = os.path.join(self.dataset_dir, "train.jsonl")
        valid_output_path = os.path.join(self.dataset_dir, "valid.jsonl")

        # åˆ†å‰²ã—ã¦æ›¸ãè¾¼ã¿
        total_train, total_valid = 0, 0
        with open(train_output_path, "w", encoding="utf-8") as train_file, \
            open(valid_output_path, "w", encoding="utf-8") as valid_file:
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            for file_path in jsonl_paths:
                if not os.path.exists(file_path):
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                    continue

                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                with open(file_path, "r", encoding="utf-8") as f:
                    lines = [line.strip() for line in f if line.strip()]

                # JSONã¨ã—ã¦æœ‰åŠ¹ãªã‚‚ã®ã ã‘æŠ½å‡º
                valid_lines = []
                for i, line in enumerate(lines, start=1):
                    try:
                        json.loads(line)
                        valid_lines.append(line)
                    except json.JSONDecodeError:
                        print(f"âš ï¸ ç„¡åŠ¹ãªJSONè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_path}:{i}")

                # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦åˆ†å‰²
                random.shuffle(valid_lines)
                split_index = int(len(valid_lines) * self.train_ratio)
                train_lines = valid_lines[:split_index]
                valid_lines = valid_lines[split_index:]

                # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
                for line in train_lines:
                    train_file.write(line + "\n")
                for line in valid_lines:
                    valid_file.write(line + "\n")

                total_train += len(train_lines)
                total_valid += len(valid_lines)

                print(f"âœ… {os.path.basename(file_path)} ã‚’åˆ†å‰²ã—ã¾ã—ãŸ "
                    f"(train: {len(train_lines)}, valid: {len(valid_lines)})")

        print(f"\nğŸ“Š ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²å®Œäº†")
        print(f"Train åˆè¨ˆ: {total_train}")
        print(f"Valid åˆè¨ˆ: {total_valid}")
        print(f"å‡ºåŠ›å…ˆ: {self.dataset_dir}")

if __name__ == "__main__":
    import datetime
    makedate = datetime.datetime.now().strftime("%Y%m%d") 

    DatasetMaker(makedate=makedate).main()